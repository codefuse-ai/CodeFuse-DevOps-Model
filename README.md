<div align="center">
<h1>
 DevOps-Model
</h1>
</div>

<p align="center">
ğŸ¤— <a href="https://huggingface.co/codefuse-ai" target="_blank">Hugging Face</a> â€¢ 
ğŸ¤– <a href="https://modelscope.cn/organization/codefuse-ai" target="_blank">ModelScope</a> </p>

<div align="center">
<h4 align="center">
    <p>
        <b>ä¸­æ–‡</b> |
        <a href="https://github.com/codefuse-ai/CodeFuse-DevOps-Model/blob/main/README_EN.md">English</a>
    <p>
</h4>
</div>

DevOps-Model æ˜¯ä¸€ç³»åˆ—ä¸šç•Œé¦–ä¸ªå¼€æºçš„**ä¸­æ–‡å¼€å‘è¿ç»´å¤§æ¨¡å‹**ï¼Œä¸»è¦è‡´åŠ›äºåœ¨ DevOps é¢†åŸŸå‘æŒ¥å®é™…ä»·å€¼ã€‚ç›®å‰ï¼ŒDevOps-Model èƒ½å¤Ÿå¸®åŠ©å·¥ç¨‹å¸ˆå›ç­”åœ¨ DevOps ç”Ÿå‘½å‘¨æœŸä¸­é‡åˆ°çš„é—®é¢˜ã€‚

æˆ‘ä»¬åŸºäº Qwen ç³»åˆ—æ¨¡å‹ï¼Œç»è¿‡é«˜è´¨é‡ä¸­æ–‡ DevOps è¯­æ–™åŠ è®­åäº§å‡º **Base** æ¨¡å‹ï¼Œç„¶åç»è¿‡ DevOps QA æ•°æ®å¯¹é½åäº§å‡º **Chat** æ¨¡å‹ã€‚æˆ‘ä»¬çš„ Base æ¨¡å‹å’Œ Chat æ¨¡å‹åœ¨å¼€æºå’Œ DevOps é¢†åŸŸç›¸å…³çš„è¯„æµ‹æ•°æ®ä¸Šå¯ä»¥å–å¾—åŒè§„æ¨¡æ¨¡å‹ä¸­çš„**æœ€ä½³æ•ˆæœ**ã€‚æ¬¢è¿æ¥æˆ‘ä»¬éƒ¨ç½²çš„åœ¨çº¿è¯•ç”¨åœ°å€è¯•ç”¨æ¨¡å‹æ•ˆæœï¼šhttps://modelscope.cn/studios/codefuse-ai/DevOps-Model-Demo/summary
<br>
<br>
åŒæ—¶æˆ‘ä»¬ä¹Ÿåœ¨æ­å»º DevOps é¢†åŸŸä¸“å±çš„è¯„æµ‹åŸºå‡† [DevOpsEval](https://github.com/codefuse-ai/codefuse-devops-eval)ï¼Œç”¨æ¥æ›´å¥½è¯„æµ‹ DevOps é¢†åŸŸæ¨¡å‹çš„æ•ˆæœã€‚
<br>
<br>

# æœ€æ–°æ¶ˆæ¯
- [2023.12.22] æˆ‘ä»¬éƒ¨ç½²äº† DevOps-Model çš„åœ¨çº¿æ¨¡å‹é—®ç­”åœ°å€ï¼Œæ¬¢è¿è¯•ç”¨ï¼ï¼ï¼ https://modelscope.cn/studios/codefuse-ai/DevOps-Model-Demo/summary
- [2023.12.06] æ›´æ–° Huggingface ä¸‹è½½åœ°å€
- [2023.10.31] å¼€æº DevOps-Model-14B Base å’Œ Chat æ¨¡å‹ã€‚
- [2023.10.30] å¼€æº DevOps-Model-7B Base å’Œ Chat æ¨¡å‹ã€‚


# æ¨¡å‹ä¸‹è½½
å¼€æºæ¨¡å‹å’Œä¸‹è½½é“¾æ¥è§ä¸‹è¡¨ï¼š
ğŸ¤— Huggingface åœ°å€

|         | åŸºåº§æ¨¡å‹  | å¯¹é½æ¨¡å‹ | å¯¹é½æ¨¡å‹ Int4 é‡åŒ– |
|:-------:|:-------:|:-------:|:-----------------:|
| 7B      |  [DevOps-Model-7B-Base](https://huggingface.co/codefuse-ai/CodeFuse-DevOps-Model-7B-Base)| [DevOps-Model-7B-Chat](https://huggingface.co/codefuse-ai/CodeFuse-DevOps-Model-7B-Chat) | Coming Soon|
| 14B     | [DevOps-Model-14B-Base](https://huggingface.co/codefuse-ai/CodeFuse-DevOps-Model-14B-Base) | [DevOps-Model-14B-Chat](https://huggingface.co/codefuse-ai/CodeFuse-DevOps-Model-14B-Chat) | Coming Soon |


ğŸ¤– ModelScope åœ°å€

|         | åŸºåº§æ¨¡å‹  | å¯¹é½æ¨¡å‹ | å¯¹é½æ¨¡å‹ Int4 é‡åŒ– |
|:-------:|:-------:|:-------:|:-----------------:|
| 7B      |  [DevOps-Model-7B-Base](https://modelscope.cn/models/codefuse-ai/CodeFuse-DevOps-Model-7B-Chat/summary) | [DevOps-Model-7B-Chat](https://modelscope.cn/models/codefuse-ai/CodeFuse-DevOps-Model-7B-Chat/summary) | Coming Soon|
| 14B     | [DevOps-Model-14B-Base](https://modelscope.cn/models/codefuse-ai/CodeFuse-DevOps-Model-14B-Base/summary) | [DevOps-Model-14B-Chat](https://modelscope.cn/models/codefuse-ai/CodeFuse-DevOps-Model-14B-Chat/summary) | Coming Soon |


# æ¨¡å‹è¯„æµ‹
æˆ‘ä»¬å…ˆé€‰å–äº† CMMLU å’Œ CEval ä¸¤ä¸ªè¯„æµ‹æ•°æ®é›†ä¸­å’Œ DevOps ç›¸å…³çš„ä¸€å…±å…­é¡¹è€ƒè¯•ã€‚æ€»è®¡ä¸€å…± 574 é“é€‰æ‹©é¢˜ï¼Œå…·ä½“ä¿¡æ¯å¦‚ä¸‹ï¼š

|  è¯„æµ‹æ•°æ®é›† | è€ƒè¯•ç§‘ç›®  | é¢˜æ•°  | 
|:-------:|:-------:|:-------:|
|   CMMLU  | Computer science | 204 |
|   CMMLU  | Computer security | 171 |
|   CMMLU  | Machine learning | 122 |
| CEval   | College programming | 37 |
| CEval   | Computer architecture | 21 |
| CEval   | Computernetwork | 19 |


æˆ‘ä»¬åˆ†åˆ«æµ‹è¯•äº† Zero-shot å’Œ Five-shot çš„ç»“æœï¼Œæˆ‘ä»¬çš„ 7B å’Œ 14B ç³»åˆ—æ¨¡å‹å¯ä»¥å–å¾—åœ¨æµ‹è¯•çš„æ¨¡å‹ä¸­æœ€å¥½çš„æˆç»©ï¼Œæ›´å¤šçš„æµ‹è¯•åç»­ä¹Ÿä¼šæ”¾å‡ºã€‚

|Base æ¨¡å‹|Zero-shot å¾—åˆ†|Five-shot å¾—åˆ†|
|:-------:|:-------:|:-------:|
|**DevOps-Model-14B-Base**| **70.73** | **73.00** |
|Qwen-14B-Base| 69.16 | 71.25  |
|Baichuan2-13B-Base| 55.75 | 61.15 |
|**DevOps-Model-7B-Base**| **62.72** | **62.02** |
|Qwen-7B-Base| 55.75 | 56.00 | 
|Baichuan2-7B-Base| 49.30 | 55.4 |
|Internlm-7B-Base| 47.56 | 52.6 |
<br>

|Chat æ¨¡å‹|Zero-shot å¾—åˆ†|Five-shot å¾—åˆ†|
|:-------:|:-------:|:-------:|
|**DevOps-Model-14B-Chat**| **74.04** | **75.96** |
|Qwen-14B-Chat| 69.16 | 70.03 |
|Baichuan2-13B-Chat| 52.79 | 55.23 |
|**DevOps-Model-7B-Chat**| **62.20** | **64.11** |
|Qwen-7B-Chat| 46.00 | 52.44 |
|Baichuan2-7B-Chat| 52.56 | 55.75 |
|Internlm-7B-Chat| 52.61 | 55.75 |

<br>
 <br>

# å¿«é€Ÿä½¿ç”¨
æˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ ğŸ¤— Transformers å¿«é€Ÿä½¿ç”¨ Devops-Model-Chat æ¨¡å‹ã€‚

## å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

## Chat æ¨¡å‹æ¨ç†ç¤ºä¾‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("path_to_DevOps-Model-Chat", trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained("path_to_DevOps-Model-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()

# æŒ‡å®š generation_config
model.generation_config = GenerationConfig.from_pretrained("path_to_DevOps-Model-Chat", trust_remote_code=True)

# ç¬¬ä¸€è½®å¯¹è¯
resp, hist = model.chat(query='ä½ æ˜¯è°', tokenizer=tokenizer, history=None)
print(resp)
# æˆ‘æ˜¯ DevOps-Modelï¼Œä¸€ä¸ªç”±èš‚èšé›†å›¢å¹³å°æŠ€æœ¯äº‹ä¸šç¾¤é£é™©æ™ºèƒ½å›¢é˜Ÿå’ŒåŒ—äº¬å¤§å­¦è”åˆç ”å‘çš„äººå·¥æ™ºèƒ½æœºå™¨äººï¼Œå¯ä»¥ä¸ç”¨æˆ·è¿›è¡Œè‡ªç„¶è¯­è¨€äº¤äº’ï¼Œå¹¶ååŠ©è§£ç­” DevOps å…¨ç”Ÿå‘½å‘¨æœŸä¸­çš„å„ç§é—®é¢˜ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•éœ€è¦ååŠ©çš„é—®é¢˜æˆ–è€…æƒ³è¦è¿›è¡Œé—²èŠï¼Œéƒ½å¯ä»¥å’Œæˆ‘äº¤æµå“¦ã€‚

# ç¬¬äºŒè½®å¯¹è¯
resp2, hist2 = model.chat(query='Java ä¸­ HashMap å’Œ Hashtable æœ‰ä»€ä¹ˆåŒºåˆ«', tokenizer=tokenizer, history=hist)
print(resp2)
# HashMap å’Œ Hashtable éƒ½æ˜¯ Java ä¸­å¸¸ç”¨çš„å“ˆå¸Œè¡¨å®ç°ï¼Œå®ƒä»¬çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼š
# 1. Hashtable æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œè€Œ HashMap ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå› æ­¤åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹ï¼ŒHashtable çš„æ€§èƒ½æ›´ç¨³å®šã€‚
# 2. Hashtable ä¸­çš„æ–¹æ³•éƒ½æ˜¯åŒæ­¥çš„ï¼Œè€Œ HashMap çš„æ–¹æ³•ä¸æ˜¯åŒæ­¥çš„ï¼Œå› æ­¤åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹ï¼ŒHashtable çš„æ€§èƒ½æ›´å¥½ã€‚
# 3. Hashtable ä¸­çš„ key å’Œ value éƒ½å¿…é¡»å®ç° Serializable æ¥å£ï¼Œè€Œ HashMap ä¸­çš„ key å’Œ value å¯ä»¥æ˜¯ä»»ä½•å¯¹è±¡ï¼ŒåŒ…æ‹¬åŸºæœ¬æ•°æ®ç±»å‹ã€‚
# 4. Hashtable çš„åˆå§‹å®¹é‡æ˜¯ 11ï¼Œè€Œ HashMap çš„åˆå§‹å®¹é‡æ˜¯ 16ã€‚
# æ€»ä¹‹ï¼Œå¦‚æœéœ€è¦åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹ä½¿ç”¨å“ˆå¸Œè¡¨ï¼Œå¹¶ä¸”éœ€è¦ä¿è¯çº¿ç¨‹å®‰å…¨ï¼Œé‚£ä¹ˆåº”è¯¥ä½¿ç”¨ Hashtableï¼›å¦‚æœä¸éœ€è¦è€ƒè™‘çº¿ç¨‹å®‰å…¨ï¼Œæˆ–è€…éœ€è¦å¿«é€Ÿåœ°è¿›è¡Œå“ˆå¸Œè¡¨æ“ä½œï¼Œé‚£ä¹ˆåº”è¯¥ä½¿ç”¨ HashMapã€‚

# ç¬¬ä¸‰è½®å¯¹è¯
resp3, hist3 = model.chat(query='çº¿ç¨‹å®‰å…¨ä»£è¡¨ä»€ä¹ˆ', tokenizer=tokenizer, history=hist2)
print(resp3)
# çº¿ç¨‹å®‰å…¨æ˜¯æŒ‡åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹ï¼Œç¨‹åºèƒ½å¤Ÿæ­£ç¡®åœ°å¤„ç†å¹¶å‘è®¿é—®ï¼Œå¹¶ä¸”ä¸ä¼šå‡ºç°æ•°æ®ç«äº‰ã€æ­»é”ã€é¥¥é¥¿ç­‰å¼‚å¸¸æƒ…å†µã€‚çº¿ç¨‹å®‰å…¨çš„ç¨‹åºå¯ä»¥ä¿è¯åœ¨ä¸åŒçš„çº¿ç¨‹ä¹‹é—´å…±äº«åŒä¸€ä¸ªæ•°æ®ç»“æ„æ—¶ï¼Œæ•°æ®çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚çº¿ç¨‹å®‰å…¨çš„å®ç°é€šå¸¸éœ€è¦ä½¿ç”¨åŒæ­¥æœºåˆ¶ï¼Œå¦‚é”ã€åŸå­æ“ä½œç­‰ï¼Œæ¥ä¿è¯å¯¹å…±äº«æ•°æ®çš„è®¿é—®æ˜¯çº¿ç¨‹å®‰å…¨çš„ã€‚åœ¨ Java ä¸­ï¼Œå¯ä»¥é€šè¿‡ synchronized å…³é”®å­—ã€Lock æ¥å£ç­‰æœºåˆ¶æ¥å®ç°çº¿ç¨‹å®‰å…¨ã€‚
```


# æ¨¡å‹è®­ç»ƒ

## æ•°æ®å‡†å¤‡
ä»£ç å†…éƒ¨é€šè¿‡è°ƒç”¨ datasets.load_dataset è¯»å–æ•°æ®ï¼Œæ”¯æŒ load_dataset æ‰€æ”¯æŒçš„æ•°æ®è¯»å–æ–¹å¼ï¼Œæ¯”å¦‚ jsonï¼Œcsvï¼Œè‡ªå®šä¹‰è¯»å–è„šæœ¬ç­‰æ–¹å¼ï¼ˆä½†æ¨èæ•°æ®å‡†å¤‡ä¸º jsonl æ ¼å¼çš„æ–‡ä»¶ï¼‰ã€‚ç„¶åè¿˜éœ€è¦æ›´æ–° `data/dataset_info.json` æ–‡ä»¶ï¼Œå…·ä½“å¯ä»¥å‚è€ƒ `data/README.md`ã€‚

## é¢„è®­ç»ƒ
å¦‚æœæ”¶é›†äº†ä¸€æ‰¹æ–‡æ¡£ä¹‹ç±»çš„è¯­æ–™ï¼ˆæ¯”å¦‚å…¬å¸å†…éƒ¨äº§å“çš„æ–‡æ¡£ï¼‰æƒ³è¦åœ¨ devopspal æ¨¡å‹ä¸ŠåŠ è®­ï¼Œå¯ä»¥æ‰§è¡Œ `scripts/devops-model-pt.sh` æ¥å‘èµ·ä¸€æ¬¡åŠ è®­æ¥è®©æ¨¡å‹å­¦ä¹ åˆ°è¿™æ‰¹æ–‡æ¡£çš„çŸ¥è¯†ï¼Œå…·ä½“ä»£ç å¦‚ä¸‹:

```bash
set -v 

torchrun --nproc_per_node=8 --nnodes=$WORLD_SIZE --master_port=$MASTER_PORT --master_addr=$MASTER_ADDR --node_rank=$RANK src/train_bash.py \
    --deepspeed conf/deepspeed_config.json \    # deepspeed é…ç½®åœ°å€
	--stage pt \    # ä»£è¡¨æ‰§è¡Œ pretrain
    --model_name_or_path path_to_model \    # huggingfaceä¸‹è½½çš„ devopspal æ¨¡å‹åœ°å€
    --do_train \
    --report_to 'tensorboard' \
    --dataset your_corpus \    # æ•°æ®é›†åå­—ï¼Œè¦å’Œåœ¨ dataset_info.json ä¸­å®šä¹‰çš„ä¸€è‡´
    --template default \    # templateï¼Œpretrain å°±æ˜¯ default
    --finetuning_type full \  # å…¨é‡æˆ–è€… lora
    --output_dir path_to_output_checkpoint_path \    # æ¨¡å‹ checkpoint ä¿å­˜çš„è·¯å¾„
    --overwrite_cache \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --lr_scheduler_type cosine \
    --warmup_ratio 0.05 \
    --evaluation_strategy steps \
    --logging_steps 10 \
    --max_steps 1000 \
    --save_steps 1000 \
    --eval_steps 1000 \
    --learning_rate 5e-6 \
    --plot_loss \
    --max_source_length=2048 \
    --dataloader_num_workers 8 \
    --val_size 0.01 \
    --bf16 \
    --overwrite_output_dir
```

ä½¿ç”¨è€…å¯ä»¥åœ¨è¿™ä¸ªåŸºç¡€ä¸Šè°ƒæ•´æ¥å‘èµ·è‡ªå·±çš„è®­ç»ƒï¼Œæ›´åŠ è¯¦ç»†çš„å¯é…ç½®é¡¹å»ºè®®é€šè¿‡ `python src/train_bash.py -h` æ¥è·å–å®Œæ•´çš„å‚æ•°åˆ—è¡¨ã€‚

## æŒ‡ä»¤å¾®è°ƒ
å¦‚æœæ”¶é›†äº†ä¸€æ‰¹ QA æ•°æ®æƒ³è¦é’ˆå¯¹ devopspal å†è¿›è¡Œå¯¹é½çš„è¯ï¼Œå¯ä»¥æ‰§è¡Œ `scripts/devops-model-sft.sh` æ¥å‘èµ·ä¸€æ¬¡åŠ è®­æ¥è®©æ¨¡å‹åœ¨æ”¶é›†åˆ°çš„æ¨¡å‹ä¸Šè¿›è¡Œå¯¹é½ï¼Œå…·ä½“ä»£ç å¦‚ä¸‹:
```bash
set -v 

torchrun --nproc_per_node=8 --nnodes=$WORLD_SIZE --master_port=$MASTER_PORT --master_addr=$MASTER_ADDR --node_rank=$RANK src/train_bash.py \
    --deepspeed conf/deepspeed_config.json \    # deepspeed é…ç½®åœ°å€
    --stage sft \    # ä»£è¡¨æ‰§è¡Œ pretrain
    --model_name_or_path path_to_model \    # huggingfaceä¸‹è½½çš„æ¨¡å‹åœ°å€
    --do_train \
    --report_to 'tensorboard' \
    --dataset your_corpus \    # æ•°æ®é›†åå­—ï¼Œè¦å’Œåœ¨ dataset_info.json ä¸­å®šä¹‰çš„ä¸€è‡´
    --template chatml \    # template qwen æ¨¡å‹å›ºå®šå†™ chatml
    --finetuning_type full \    # å…¨é‡æˆ–è€… lora
    --output_dir /mnt/llm/devopspal/model/trained \     # æ¨¡å‹ checkpoint ä¿å­˜çš„è·¯å¾„
    --overwrite_cache \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --lr_scheduler_type cosine \
    --warmup_ratio 0.05 \
    --evaluation_strategy steps \
    --logging_steps 10 \
    --max_steps 1000 \
    --save_steps 100 \
    --eval_steps 100 \
    --learning_rate 5e-5 \
    --plot_loss \
    --max_source_length=2048 \
    --dataloader_num_workers 8 \
    --val_size 0.01 \
    --bf16 \
    --overwrite_output_dir
```

ä½¿ç”¨è€…å¯ä»¥åœ¨è¿™ä¸ªåŸºç¡€ä¸Šè°ƒæ•´æ¥å‘èµ·è‡ªå·±çš„ SFT è®­ç»ƒï¼Œæ›´åŠ è¯¦ç»†çš„å¯é…ç½®é¡¹å»ºè®®é€šè¿‡ `python src/train_bash.py -h` æ¥è·å–å®Œæ•´çš„å‚æ•°åˆ—è¡¨ã€‚

## é‡åŒ–
æˆ‘ä»¬å°†ä¼šæä¾›äº† DevOps-Model-Chat ç³»åˆ—çš„é‡åŒ–æ¨¡å‹ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç æ¥é‡åŒ–è‡ªå·±åŠ è®­è¿‡çš„æ¨¡å‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from optimum.gptq import GPTQQuantizer, load_quantized_model
import torch

# åŠ è½½æ¨¡å‹
model_name = "path_of_your_model"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)

# åŠ è½½æ•°æ®
# todo

# å¼€å§‹é‡åŒ–
quantizer = GPTQQuantizer(bits=4, dataset="c4", block_name_to_quantize = "model.decoder.layers", model_seqlen = 2048)
quantized_model = quantizer.quantize_model(model, tokenizer)

# ä¿å­˜é‡åŒ–åçš„æ¨¡å‹
out_dir = 'save_path_of_your_quantized_model'
quantized_model.save_quantized(out_dir)
```


# è”ç³»æˆ‘ä»¬
![](https://github.com/codefuse-ai/CodeFuse-DevOps-Model/blob/main/imgs/wechat2.png)


# å…è´£å£°æ˜
ç”±äºè¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ï¼Œæ¨¡å‹ç”Ÿæˆçš„å†…å®¹å¯èƒ½åŒ…å«å¹»è§‰æˆ–è€…æ­§è§†æ€§è¨€è®ºã€‚è¯·è°¨æ…ä½¿ç”¨ DevOps-Model ç³»åˆ—æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ã€‚
å¦‚æœè¦å…¬å¼€ä½¿ç”¨æˆ–å•†ç”¨è¯¥æ¨¡å‹æœåŠ¡ï¼Œè¯·æ³¨æ„æœåŠ¡æ–¹éœ€æ‰¿æ‹…ç”±æ­¤äº§ç”Ÿçš„ä¸è‰¯å½±å“æˆ–æœ‰å®³è¨€è®ºçš„è´£ä»»ï¼Œæœ¬é¡¹ç›®å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•ç”±ä½¿ç”¨æœ¬é¡¹ç›®ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®ã€æ¨¡å‹ã€ä»£ç ç­‰ï¼‰å¯¼è‡´çš„å±å®³æˆ–æŸå¤±ã€‚

# å¼•ç”¨
å¦‚æœä½¿ç”¨æœ¬é¡¹ç›®çš„ä»£ç æˆ–æ¨¡å‹ï¼Œè¯·å¼•ç”¨æœ¬é¡¹ç›®è®ºæ–‡ï¼š

é“¾æ¥ï¼š[DevOps-Model](https://arxiv.org)

```
@article{devopspal2023,
  title={},
  author={},
  journal={arXiv preprint arXiv},
  year={2023}
}
```

# è‡´è°¢
æœ¬é¡¹ç›®å‚è€ƒäº†ä»¥ä¸‹å¼€æºé¡¹ç›®ï¼Œåœ¨æ­¤å¯¹ç›¸å…³é¡¹ç›®å’Œç ”ç©¶å¼€å‘äººå‘˜è¡¨ç¤ºæ„Ÿè°¢ã€‚
- [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning)
- [QwenLM](https://github.com/QwenLM)
